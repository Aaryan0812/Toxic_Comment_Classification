{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcGq1dF0iWju"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31370,
     "status": "ok",
     "timestamp": 1652102149294,
     "user": {
      "displayName": "Harsh Attree",
      "userId": "01430067086563950496"
     },
     "user_tz": -330
    },
    "id": "iyTgJx85_MUt",
    "outputId": "9c0ba5d6-f774-4f0a-a294-eca9ef6e349f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: talos in d:\\new folder\\lib\\site-packages (1.3)\n",
      "Requirement already satisfied: astetik in d:\\new folder\\lib\\site-packages (from talos) (1.13)\n",
      "Requirement already satisfied: chances in d:\\new folder\\lib\\site-packages (from talos) (0.1.9)\n",
      "Requirement already satisfied: kerasplotlib in d:\\new folder\\lib\\site-packages (from talos) (1.0)\n",
      "Requirement already satisfied: statsmodels>=0.11.0 in d:\\new folder\\lib\\site-packages (from talos) (0.12.2)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in d:\\new folder\\lib\\site-packages (from talos) (2.12.0)\n",
      "Requirement already satisfied: sklearn in d:\\new folder\\lib\\site-packages (from talos) (0.0.post1)\n",
      "Requirement already satisfied: numpy in d:\\new folder\\lib\\site-packages (from talos) (1.22.4)\n",
      "Requirement already satisfied: tqdm in d:\\new folder\\lib\\site-packages (from talos) (4.59.0)\n",
      "Requirement already satisfied: requests in d:\\new folder\\lib\\site-packages (from talos) (2.25.1)\n",
      "Requirement already satisfied: wrangle in d:\\new folder\\lib\\site-packages (from talos) (0.7.2)\n",
      "Requirement already satisfied: pandas in d:\\new folder\\lib\\site-packages (from talos) (1.2.4)\n",
      "Requirement already satisfied: scipy>=1.1 in d:\\new folder\\lib\\site-packages (from statsmodels>=0.11.0->talos) (1.6.2)\n",
      "Requirement already satisfied: patsy>=0.5 in d:\\new folder\\lib\\site-packages (from statsmodels>=0.11.0->talos) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\new folder\\lib\\site-packages (from pandas->talos) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\new folder\\lib\\site-packages (from pandas->talos) (2.8.1)\n",
      "Requirement already satisfied: six in d:\\new folder\\lib\\site-packages (from patsy>=0.5->statsmodels>=0.11.0->talos) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in d:\\new folder\\lib\\site-packages (from tensorflow>=2.0.0->talos) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (1.4.0)\n",
      "Requirement already satisfied: setuptools in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (52.0.0.post20210125)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.4.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.4.6)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (23.3.3)\n",
      "Requirement already satisfied: packaging in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (20.9)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (2.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (1.51.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (4.22.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (16.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.31.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in d:\\new folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\new folder\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (2.16.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\new folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (3.4.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\new folder\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\new folder\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\new folder\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\new folder\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\new folder\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (6.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\new folder\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\new folder\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\new folder\\lib\\site-packages (from requests->talos) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\new folder\\lib\\site-packages (from requests->talos) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\new folder\\lib\\site-packages (from requests->talos) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\new folder\\lib\\site-packages (from requests->talos) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\new folder\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.0.0->talos) (3.2.2)\n",
      "Requirement already satisfied: seaborn in d:\\new folder\\lib\\site-packages (from astetik->talos) (0.11.1)\n",
      "Requirement already satisfied: IPython in d:\\new folder\\lib\\site-packages (from astetik->talos) (7.22.0)\n",
      "Requirement already satisfied: geonamescache in d:\\new folder\\lib\\site-packages (from astetik->talos) (1.5.0)\n",
      "Requirement already satisfied: backcall in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (0.2.0)\n",
      "Requirement already satisfied: pygments in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (2.8.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (0.17.2)\n",
      "Requirement already satisfied: colorama in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (0.4.6)\n",
      "Requirement already satisfied: pickleshare in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (3.0.17)\n",
      "Requirement already satisfied: traitlets>=4.2 in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (5.0.5)\n",
      "Requirement already satisfied: decorator in d:\\new folder\\lib\\site-packages (from IPython->astetik->talos) (5.0.6)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in d:\\new folder\\lib\\site-packages (from jedi>=0.16->IPython->astetik->talos) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in d:\\new folder\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->astetik->talos) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in d:\\new folder\\lib\\site-packages (from traitlets>=4.2->IPython->astetik->talos) (0.2.0)\n",
      "Requirement already satisfied: matplotlib in d:\\new folder\\lib\\site-packages (from kerasplotlib->talos) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\new folder\\lib\\site-packages (from matplotlib->kerasplotlib->talos) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\new folder\\lib\\site-packages (from matplotlib->kerasplotlib->talos) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\new folder\\lib\\site-packages (from matplotlib->kerasplotlib->talos) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\new folder\\lib\\site-packages (from matplotlib->kerasplotlib->talos) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aaryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "!pip install talos\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from string import ascii_lowercase\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import reduce\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8KPqb7_vC59"
   },
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 803118,
     "status": "ok",
     "timestamp": 1652102952352,
     "user": {
      "displayName": "Harsh Attree",
      "userId": "01430067086563950496"
     },
     "user_tz": -330
    },
    "id": "96Z6cXAZiaAY",
    "outputId": "449085e9-66ce-4e46-f362-fe08975b69cc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8884be70d23e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muploaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#from google.colab import files\n",
    " \n",
    " \n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yTbZDdkc1lxE"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('jigsaw-toxic-comment-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O2_gDuPMvQHZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uploadedtest = files.upload()\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vewUHsrQvcti"
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hAcigh3Sh3"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsOBGKJI4aHT"
   },
   "source": [
    "Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZzAnL7ly1epE",
    "outputId": "338d8eaa-8e84-462a-c259-130616f75ce5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               False\n",
       "comment_text     False\n",
       "toxic            False\n",
       "severe_toxic     False\n",
       "obscene          False\n",
       "threat           False\n",
       "insult           False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cRc5Wyw24c62",
    "outputId": "1a093cb4-a298-4004-da49-5081219c1f64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         False\n",
       "content    False\n",
       "lang       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TYZ93LlRbwRB"
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[labels].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDWRHguXxOF_"
   },
   "source": [
    "#Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z-4hq4jyhBw"
   },
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4k-vI8DbqiP"
   },
   "source": [
    "* Removing Characters in between Text\n",
    "* Removing Repeated Characters\n",
    "* Converting data to lower-case\n",
    "* Removing Numbers from the data\n",
    "* Remove Punctuation\n",
    "* Remove Whitespaces\n",
    "* Removing spaces in between words\n",
    "* Removing \"\\n\"\n",
    "* Remove Non-english characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jglnA7LEbpoA"
   },
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
    "            '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' trans gender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homo sex ual':\n",
    "        [\n",
    "            'homosexual'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                      'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu', 'st*u'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "    ' fucking ':\n",
    "        [\n",
    "            'f*$%-ing'\n",
    "        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "G32H2jt-J_vz"
   },
   "outputs": [],
   "source": [
    "#Ascii Removal step\n",
    "def asciiremoval(string):\n",
    "    string = re.sub('[^a-zA-Z.\\d\\s]', '', string)\n",
    "    return string\n",
    "\n",
    "#Removing dates\n",
    "def remove_dates(sentence):\n",
    "    for i in range(0,277):\n",
    "        df1[i] = df1[i].sub('(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{2}\\s\\d{4}', ' ', sentence)\n",
    "\n",
    "#Removing Url\n",
    "def remove_url(txt):\n",
    "    txt_nourl=re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', txt)\n",
    "    return txt_nourl\n",
    "\n",
    "#Punctuation Removal\n",
    "def removepunct(txt):\n",
    "    txt_nopunct=\"\".join((c for c in txt if c not in string.punctuation))\n",
    "    return txt_nopunct\n",
    "\n",
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4RetJ1vgswOG"
   },
   "outputs": [],
   "source": [
    "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "\n",
    "  if is_lower:\n",
    "    text=text.lower()\n",
    "    \n",
    "  if remove_patterns_text:\n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "      for pat in patterns:\n",
    "        text=str(text).replace(pat, target)\n",
    "    \n",
    "\n",
    "  if remove_repeat_text:\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "  text = str(text).replace(\"\\n\", \" \")\n",
    "#Ascii Removal\n",
    "  text = re.sub('[^a-zA-Z.\\d\\s]', '', text)\n",
    "#URL removal\n",
    "  text=re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', text)\n",
    "#Remove all the special characters \n",
    "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "  text = re.sub('[0-9]',\"\",text)\n",
    "  text = re.sub(\" +\", \" \", text)\n",
    "#Date Removal \n",
    "  text=re.sub('(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{2}\\s\\d{4}', ' ', text)\n",
    "  text= re.sub('[^a-zA-Z.\\d\\s]', '', text)\n",
    "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "  return text \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwyO_GcPuPE_"
   },
   "source": [
    "Cleaning Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Cak-Q0fl0qyb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daww he matches this background colour im seemingly stuck with thanks talk january utc'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text']=train['comment_text'].apply(lambda x: clean_text(x))\n",
    "train['comment_text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qfbjYXq8huL"
   },
   "source": [
    "Cleaning Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "V02zaxGN8g-s",
    "outputId": "108dabf3-0cda-4982-e94b-47ec8f5cabf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'franchement c est fastidieux de perdre du temps avec un contributeur qui visibelement joue au con avec moi depuis des mois en se livrant un pov pushing pourtant d avance vou l chec face des sources unanimes je ne peux plus supposer la bonne foi dans ces conditions merci d arrter de nous faire perdre du temps apollofox discuter '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['content']=test['content'].apply(lambda x: clean_text(x))\n",
    "test['content'][1048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PcJrI9OeLyQe"
   },
   "outputs": [],
   "source": [
    "train.to_csv('Data/train_preprocessed_first.csv')\n",
    "test.to_csv('Data/test_preprocessed_first.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4-23iVqMg10Y"
   },
   "outputs": [],
   "source": [
    "#test=pd.read_csv('Data/train_preprocessed_first.csv')\n",
    "#train=pd.read_csv('Data/test_preprocessed_first.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqRVmH1FRWpL"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vnhCAXkKUF9i"
   },
   "outputs": [],
   "source": [
    "comments_train=train['comment_text']\n",
    "comments_test=test['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WwhTNF3pW7Hp"
   },
   "outputs": [],
   "source": [
    "comments_train=list(comments_train)\n",
    "comments_test=list(comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IuWMc1xqRVqV"
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fi-vYUcPoi-a"
   },
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "#Pos Tagging-Noun,verb,adjective,adverb\n",
    "def lemma(text, lemmatization=True):\n",
    "  output=\"\"\n",
    "  if lemmatization:\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "       word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
    "       word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "       word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
    "       word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
    "       output=output + \" \" + word4\n",
    "  else:\n",
    "    output=text\n",
    "  \n",
    "  return str(output.strip()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX4DTjGJ-8Id"
   },
   "source": [
    "Lemmatizing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4c16a2137b724260bc79184ee5a7eac6"
     ]
    },
    "id": "JRquFt30qWk0",
    "outputId": "0cd7e74c-8676-4538-f38a-8f36a80e3d75"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a583bcb0af8c4b2189f5abdeaa091cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized_train_data = [] \n",
    "\n",
    "for line in tqdm_notebook(comments_train, total=159571): \n",
    "    lemmatized_train_data.append(lemma(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WK9azHjSrbic",
    "outputId": "6bb2318f-f35d-4835-c96e-78e9121c2fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i agree with you about graemels intention he be a revert nazi and wikipedia ha a way of protect against this report him to the wprr threerevertrule notice board post by report the revert nazi graemel ha show himselfherself to be a revert nazi this be unacceptable on a site that be make by it user and not it admins if you feel you have be unjustly revert more than time over a hour period please report himher to the threerevertnoticeboard wprr it be time to take back what be ours'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_train_data[152458]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxRJbMBQ-oQ-"
   },
   "source": [
    "Lemmatizing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "7709928fafe94bae9842548edf2b7c6c"
     ]
    },
    "id": "B4LZUmqy-vvD",
    "outputId": "4743a56a-3cd6-4f66-9eba-c561170aed14"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3668d0485c6b447cbc6cf25955324c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized_test_data = [] \n",
    "\n",
    "for line in tqdm_notebook(comments_test, total=len(comments_test)): \n",
    "    lemmatized_test_data.append(lemma(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZaP8BH2UG0N"
   },
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vNKjE4Vq8kpG"
   },
   "outputs": [],
   "source": [
    "stopword_list=STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_oqV08mMWmk"
   },
   "source": [
    "Adding Single and Dual to STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "go7RVS52I1US"
   },
   "outputs": [],
   "source": [
    "def iter_all_strings():\n",
    "    for size in itertools.count(1):\n",
    "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "            yield \"\".join(s)\n",
    "\n",
    "dual_alpha_list=[]\n",
    "for s in iter_all_strings():\n",
    "    dual_alpha_list.append(s)\n",
    "    if s == 'zz':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7_x3bQ4MIGVK"
   },
   "outputs": [],
   "source": [
    "dual_alpha_list.remove('i')\n",
    "dual_alpha_list.remove('a')\n",
    "dual_alpha_list.remove('am')\n",
    "dual_alpha_list.remove('an')\n",
    "dual_alpha_list.remove('as')\n",
    "dual_alpha_list.remove('at')\n",
    "dual_alpha_list.remove('be')\n",
    "dual_alpha_list.remove('by')\n",
    "dual_alpha_list.remove('do')\n",
    "dual_alpha_list.remove('go')\n",
    "dual_alpha_list.remove('he')\n",
    "dual_alpha_list.remove('hi')\n",
    "dual_alpha_list.remove('if')\n",
    "dual_alpha_list.remove('is')\n",
    "dual_alpha_list.remove('in')\n",
    "dual_alpha_list.remove('me')\n",
    "dual_alpha_list.remove('my')\n",
    "dual_alpha_list.remove('no')\n",
    "dual_alpha_list.remove('of')\n",
    "dual_alpha_list.remove('on')\n",
    "dual_alpha_list.remove('or')\n",
    "dual_alpha_list.remove('ok')\n",
    "dual_alpha_list.remove('so')\n",
    "dual_alpha_list.remove('to')\n",
    "dual_alpha_list.remove('up')\n",
    "dual_alpha_list.remove('us')\n",
    "dual_alpha_list.remove('we')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "H11kkMtXMyct",
    "outputId": "85a80e14-9aea-422b-f91c-eaf95fb15ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "for letter in dual_alpha_list:\n",
    "    stopword_list.add(letter)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ti67XuLCNnsL"
   },
   "source": [
    "Checking for other words that we may need in STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z4lBqjcBaDVK"
   },
   "outputs": [],
   "source": [
    "def search_stopwords(data, search_stop=True):\n",
    "  output=\"\"\n",
    "  if search_stop:\n",
    "    data=data.split(\" \")\n",
    "    for word in data:\n",
    "      if not word in stopword_list:\n",
    "        output=output+\" \"+word \n",
    "  else:\n",
    "    output=data\n",
    "\n",
    "  return str(output.strip())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "40799906b3fe43a887bd35f402aeae14"
     ]
    },
    "id": "xn21RMeIbQti",
    "outputId": "c28b9e28-fb15-4020-aedd-1dd0a807aedc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982f177365a84854bf7833e8811f6de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "potential_stopwords = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
    "    potential_stopwords.append(search_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "R4Yp_YRoap6Q",
    "outputId": "a29cc3a4-c54d-4e1f-c927-7195e2307ce7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223549"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(potential_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFSkmGjAZuR3"
   },
   "source": [
    "Combining all the sentences in the list into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zpmze-Q7BrK0"
   },
   "outputs": [],
   "source": [
    "def string_combine(stopword):\n",
    "  final_a=\"\"\n",
    "  for item in range(1,159571):\n",
    "    final_a=final_a+\" \"+stopword[item]\n",
    "  return final_a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UqBc4rOahmAz"
   },
   "outputs": [],
   "source": [
    "total_string_potential=string_combine(potential_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLpFRACQg10h"
   },
   "outputs": [],
   "source": [
    "#print(potential_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbEm3D8txMSH"
   },
   "source": [
    "Counting the number of words in each of the 4 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "O_hhbMS_xR6t"
   },
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bsh0BuT9xSDg"
   },
   "outputs": [],
   "source": [
    "total_string_potential_dict=word_count(total_string_potential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shmHkLSk3Xxp"
   },
   "source": [
    "Converting Dictionaries to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pSJXHnv6y9T0"
   },
   "outputs": [],
   "source": [
    "total_string_potential_df = pd.DataFrame(list(total_string_potential_dict.items()),columns = ['Word','Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjWO0vj2yaW7"
   },
   "source": [
    "Getting Dataframe output in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bzThNOjA2pjx",
    "outputId": "2493389e-037c-477b-d737-a07465837e8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daww</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>match</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>background</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>colour</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seemingly</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195350</th>\n",
       "      <td>nearendless</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195351</th>\n",
       "      <td>imagebarackobamamother</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195352</th>\n",
       "      <td>hanumakonda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195353</th>\n",
       "      <td>automaker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195354</th>\n",
       "      <td>ciu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195355 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Word  Count\n",
       "0                         daww      1\n",
       "1                        match   1062\n",
       "2                   background    770\n",
       "3                       colour    335\n",
       "4                    seemingly    165\n",
       "...                        ...    ...\n",
       "195350             nearendless      1\n",
       "195351  imagebarackobamamother      2\n",
       "195352             hanumakonda      1\n",
       "195353               automaker      1\n",
       "195354                     ciu      1\n",
       "\n",
       "[195355 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_string_potential_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX3lIQj0l36i"
   },
   "source": [
    "Retaining certain words and removing others from the above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "zacbW5ASjN2r"
   },
   "outputs": [],
   "source": [
    "potential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-yjjvj3LpZs"
   },
   "source": [
    "Adding above retrived words into the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "73PbxjxbLvz9",
    "outputId": "f5eb83da-afb9-4332-b996-7a042253ebf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "for word in potential_stopwords:\n",
    "    stopword_list.add(word)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "101v0iZjaNLR"
   },
   "source": [
    "Removing Stopwords from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RRSga8PKsktA"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, remove_stop=True):\n",
    "  output = \"\"\n",
    "  if remove_stop:\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "      if word not in stopword_list:\n",
    "        output=output + \" \" + word\n",
    "  else :\n",
    "    output=text\n",
    "\n",
    "  return str(output.strip())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1efc434bdd444acfa4bf7a8c349ffdf3"
     ]
    },
    "id": "Q-hItiV7skoV",
    "outputId": "50584003-4b08-47e3-8194-738c9b93311c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7467d34c344946e7bf1596c04c43d2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_train_data = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
    "    processed_train_data.append(remove_stopwords(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "a29qOSPA-rbS",
    "outputId": "93cd9b3d-e2a6-4d4a-a0bb-e83b33e6e5e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agree graemels intention revert nazi protect report wprr threerevertrule notice board post report revert nazi graemel himselfherself revert nazi unacceptable site admins feel unjustly revert hour period report himher threerevertnoticeboard wprr'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_data[152458]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqXHFaSC-Bkf"
   },
   "source": [
    "Removing Stopwords from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "22c74b711d8947da91e0836495ca99dd"
     ]
    },
    "id": "EQ_lUOej-H_b",
    "outputId": "da37f286-e4a9-494f-cb9a-f362b02fcb79"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1069dd992f7f486880797a99153a2523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_test_data = [] \n",
    "\n",
    "for line in tqdm_notebook(lemmatized_test_data, total=153164): \n",
    "    processed_test_data.append(remove_stopwords(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeScalvZDEdD"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "SkK1P-CdX_0N"
   },
   "outputs": [],
   "source": [
    "max_features=100000      \n",
    "maxpadlen = 200          \n",
    "val_split = 0.2      \n",
    "embedding_dim_fasttext = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIeovLIr6aAo"
   },
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qaA52PlK4xnV"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(processed_train_data))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "pkSXj5sT7jbR",
    "outputId": "336a4ba3-5130-4ada-fac1-de63fd35ab0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Vocabulary:  263853\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer.word_index\n",
    "print(\"Words in Vocabulary: \",len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfDwmzPj8TJf"
   },
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "gEmacO337twa"
   },
   "outputs": [],
   "source": [
    "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n",
    "X_te=pad_sequences(list_tokenized_test, maxlen=maxpadlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "U6dAeWAnM4RL",
    "outputId": "222f360a-f09c-4fa9-f875-cf11826d8fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: \n",
      " [  148   557 40917   240   197 40917   240    96    17   590    17   148\n",
      "   359   557     1  1108   148   331  5462   148   151    17   378   270\n",
      "   359   557    17     1  1343   148    17   378     2    32   148   557\n",
      "   197   148   181    47    91   590   148   557    17     2   489   130\n",
      "    10  1095   453  1093    17  2449   503    35   148    17   197   396\n",
      "   359    11   274   197   227   161   118    21    19    29   181    78\n",
      "    19     3   428  4765     5 40917   240   197 40917   240    96   271\n",
      "   378   303    55   590    37    42    78   427  1501    37   271     7\n",
      "   590  1319    78  1717   137    10   137  4994   137    93    42   519\n",
      "  1233    16    78  1379   115   137  1689    78  1166     5   504    78\n",
      "   478    36   271    55  3528   166  2233   466    36   308  1664    66\n",
      "   181   237   161  2682    36    36  1674    78  5209  1674    78    36\n",
      "    78    36   197   271    47    91   590     5    36   271   197    77\n",
      "    10   842  3323    17    11   274    36   227   161   118    21    17\n",
      "    78   971   478  2682    17    11   374    19    29   181    78    19\n",
      "     3   428     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "One hot label: \n",
      " [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sentences: \\n', X_t[10])\n",
    "print('One hot label: \\n', y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "XtZD1j9BCrsG"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(X_t.shape[0])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "k51hkGqOnZLB",
    "outputId": "b4a13f65-bc61-4053-d52a-afef29cfcaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are : \n",
      " [[ 232   31   39 ...    0    0    0]\n",
      " [9620 3355 1287 ...    0    0    0]\n",
      " [ 520  119    8 ...    0    0    0]\n",
      " ...\n",
      " [ 316 2745  579 ...    0    0    0]\n",
      " [ 381  422 2378 ...    0    0    0]\n",
      " [4118  405  126 ...    0    0    0]]\n",
      "labels are: \n",
      " [[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 1 0 1 0]\n",
      " ...\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_t = X_t[indices]\n",
    "labels = y[indices]\n",
    "print('Features are :','\\n',X_t)\n",
    "print('labels are:','\\n',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWn-SkJCCswd"
   },
   "source": [
    "### Splitting data into Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "B5hOSGJVb7o4"
   },
   "outputs": [],
   "source": [
    "num_validation_samples = int(val_split*X_t.shape[0])\n",
    "x_train = X_t[: -num_validation_samples]\n",
    "y_train = labels[: -num_validation_samples]\n",
    "x_val = X_t[-num_validation_samples: ]\n",
    "y_val = labels[-num_validation_samples: ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "cHyYFZPDcEmK",
    "outputId": "76db6d2d-8650-471e-ccc6-da521abeaf37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in each category:\n",
      "training:  [17094  1585  9689   561  9024  1704]\n",
      "validation:  [4290  377 2451  128 2280  413]\n"
     ]
    }
   ],
   "source": [
    "print('Number of entries in each category:')\n",
    "print('training: ', y_train.sum(axis=0))\n",
    "print('validation: ', y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL7Mv1Z6D8_w"
   },
   "source": [
    "### Importing Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "hWWV1nWhg10t"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-c8021697f36b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mzip_file_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_file_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\New folder\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "zip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
    "\n",
    "r = requests.get(zip_file_url)\n",
    "\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "z.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsvNLXZ6DlWS"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "embeddings_index_fasttext = {}\n",
    "\n",
    "f = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "\n",
    "\n",
    "for line in f:\n",
    "\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "\n",
    "    word = values[0]\n",
    "\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "    embeddings_index_fasttext[word] = coefs\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "hpkSOB6iEFCB",
    "outputId": "e65ab8c2-68de-48af-d3e2-dc1909d47075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n",
      "number of null word embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index_fasttext.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fasttext[i] = embedding_vector\n",
    "        \n",
    "print(\"Completed!\")\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix_fasttext, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OoZ7fTpPWhi"
   },
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "l_Lvj1UUg10u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-bert in d:\\new folder\\lib\\site-packages (0.89.0)\n",
      "Requirement already satisfied: numpy in d:\\new folder\\lib\\site-packages (from keras-bert) (1.22.4)\n",
      "Requirement already satisfied: keras-transformer==0.40.0 in d:\\new folder\\lib\\site-packages (from keras-bert) (0.40.0)\n",
      "Requirement already satisfied: keras-layer-normalization==0.16.0 in d:\\new folder\\lib\\site-packages (from keras-transformer==0.40.0->keras-bert) (0.16.0)\n",
      "Requirement already satisfied: keras-embed-sim==0.10.0 in d:\\new folder\\lib\\site-packages (from keras-transformer==0.40.0->keras-bert) (0.10.0)\n",
      "Requirement already satisfied: keras-pos-embd==0.13.0 in d:\\new folder\\lib\\site-packages (from keras-transformer==0.40.0->keras-bert) (0.13.0)\n",
      "Requirement already satisfied: keras-multi-head==0.29.0 in d:\\new folder\\lib\\site-packages (from keras-transformer==0.40.0->keras-bert) (0.29.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward==0.8.0 in d:\\new folder\\lib\\site-packages (from keras-transformer==0.40.0->keras-bert) (0.8.0)\n",
      "Requirement already satisfied: keras-self-attention==0.51.0 in d:\\new folder\\lib\\site-packages (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras-bert) (0.51.0)\n",
      "Requirement already satisfied: keras-rectified-adam in d:\\new folder\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy in d:\\new folder\\lib\\site-packages (from keras-rectified-adam) (1.22.4)\n",
      "Requirement already satisfied: Keras in d:\\new folder\\lib\\site-packages (from keras-rectified-adam) (2.12.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras-bert\n",
    "# !pip install keras-rectified-adam\n",
    "#!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "#!unzip -o uncased_L-12_H-768_A-12.zip\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import concatenate\n",
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Embedding, SpatialDropout1D, Bidirectional, GRU, GlobalMaxPooling1D, Dense\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "2Xmp_Rs6g10v"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LR = 1e-4\n",
    "import os\n",
    "pretrained_path = r'D:\\Aaryan project\\Toxic-comment\\uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "bert_config_file=r'D:\\Aaryan project\\Toxic-comment\\uncased_L-12_H-768_A-12\\bert_config.json'\n",
    "#bert_ckpt_file=r'C:\\Users\\RD\\Toxic-Comment-Classifier-using-Deep-Learning\\uncased_L-12_H-768_A-12\\bert_config.json'\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "fsrbYlK4g10v"
   },
   "outputs": [],
   "source": [
    "inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "def build_model():\n",
    "    inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "    from tensorflow.keras.layers import BatchNormalization\n",
    "#     from tensorflow.keras.layers import Input,Dense,Lambda\n",
    "    model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        checkpoint_path,\n",
    "        training=True,\n",
    "        trainable=True,\n",
    "        seq_len=SEQ_LEN,\n",
    "    )\n",
    "#     inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "    inputs = model.inputs[:2]\n",
    "    dense = model.layers[-3].output\n",
    "    dense2 = keras.layers.Dense(10,activation='relu', kernel_initializer ='glorot_uniform')(dense)\n",
    "    dense3 = keras.layers.Dense(10,activation='relu', kernel_initializer ='glorot_uniform')(dense2)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform',\n",
    "                                 name = 'real_output')(dense3)\n",
    "\n",
    "    \n",
    "   \n",
    "    model = keras.models.Model(inputs,outputs)\n",
    "       \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e02fccd7fdc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_bert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m model_bert.compile(optimizer='rmsprop',\n\u001b[0;32m      3\u001b[0m               \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_bert = build_model()\n",
    "model_bert.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "oZdik3-jGkye"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# from bert import BertModelLayer\n",
    "# from bert.loader import StockBertConfig, load_stock_weights\n",
    "# from bert.loader import map_to_stock_variable_name\n",
    "\n",
    "# model_bert = build_model()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "2mQ7Y_QPg10x",
    "outputId": "39e653d1-e5eb-46c9-879d-8e838dc994d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_37\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input-Token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Embedding-Token (TokenEmbeddin  [(None, 128, 768),  23440896    ['Input-Token[0][0]']            \n",
      " g)                              (30522, 768)]                                                    \n",
      "                                                                                                  \n",
      " Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n",
      "                                                                                                  \n",
      " Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n",
      "                                                                  'Embedding-Segment[0][0]']      \n",
      "                                                                                                  \n",
      " Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n",
      "                                                                                                  \n",
      " Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion (MultiHeadAttention)                                        ]']                              \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion-Add (Add)                                                   ]',                              \n",
      "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-10-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-11-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-12-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 10)           7690        ['NSP-Dense[0][0]']              \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 10)           110         ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " real_output (Dense)            (None, 1)            11          ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,195,139\n",
      "Trainable params: 109,195,139\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHrPeYI_Giky"
   },
   "source": [
    "#### Training Model with Best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIRoKKp9mzQz"
   },
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "U-fPTQKHFF61"
   },
   "outputs": [],
   "source": [
    "inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           embedding_dim_fasttext,\n",
    "                           weights = [embedding_matrix_fasttext],\n",
    "                           input_length = maxpadlen,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_47\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input-Token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Embedding-Token (TokenEmbeddin  [(None, 128, 768),  23440896    ['Input-Token[0][0]']            \n",
      " g)                              (30522, 768)]                                                    \n",
      "                                                                                                  \n",
      " Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n",
      "                                                                                                  \n",
      " Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n",
      "                                                                  'Embedding-Segment[0][0]']      \n",
      "                                                                                                  \n",
      " Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n",
      "                                                                                                  \n",
      " Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion (MultiHeadAttention)                                        ]']                              \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion-Add (Add)                                                   ]',                              \n",
      "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-10-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-11-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-12-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 10)           7690        ['NSP-Dense[0][0]']              \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 10)           110         ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " real_output (Dense)            (None, 1)            11          ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,195,139\n",
      "Trainable params: 109,195,139\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iSqW0thULSae",
    "outputId": "c44f1006-e47c-4d1e-c7de-2aa7bb1cf3d6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f8da2e568cb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model_info_1=model_bert.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_info_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_bert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_bert' is not defined"
     ]
    }
   ],
   "source": [
    "# model_info_1=model_bert.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))\n",
    "model_info_1=model_bert.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL2OV0YiB3sp"
   },
   "source": [
    "## Plotting Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK9KghRBLlq9"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qKSCDX-KjSX"
   },
   "outputs": [],
   "source": [
    "loss = model_info_1.history['loss']\n",
    "val_loss = model_info_1.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zio5y-3yRn-6"
   },
   "outputs": [],
   "source": [
    "accuracy = model_info_1.history['accuracy']\n",
    "val_accuracy = model_info_1.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, accuracy, label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wv1IdwZk_qG"
   },
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eisFdjOUlBbM",
    "outputId": "261fc81a-c0f6-49f6-ffa7-dc515b4efeb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Data/model_BERT\\assets\n"
     ]
    }
   ],
   "source": [
    "model_bert.save('Data/model_BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXx4IOYn4_XB"
   },
   "source": [
    "# Loading Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huVktHqRMSGc"
   },
   "outputs": [],
   "source": [
    "loaded_model_bert = keras.models.load_model('Data/model_BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJJJs5txVS8S"
   },
   "source": [
    "# Generating the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBk_wYFWMY82"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUL3S8xiCYxU",
    "outputId": "27396cc7-7c44-4b73-d3a7-948b571cec1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 39546s 258ms/step\n"
     ]
    }
   ],
   "source": [
    "test_values_1 = loaded_model_bert.predict([X_te], batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FSvu4hpyU4r"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(r'C:\\Users\\RD\\Toxic-Comment-Classifier-using-Deep-Learning\\Data\\sample_submission.csv')\n",
    "test_values_1=pd.DataFrame(test_values_1,columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
    "submission = pd.DataFrame(sample_submission[\"id\"])\n",
    "combined_submission=pd.concat([submission,test_values_1],axis=1)\n",
    "combined_submission.to_csv(r'C:\\Users\\RD\\Toxic-Comment-Classifier-using-Deep-Learning\\Data\\combined_submission_bert_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HkAldq1IJcr"
   },
   "source": [
    "# Testing the Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPneHbuzIIxj"
   },
   "outputs": [],
   "source": [
    "def toxicity_level(string):\n",
    "    new_string = [string]\n",
    "    new_string = tokenizer.texts_to_sequences(new_string)\n",
    "    new_string = pad_sequences(new_string, maxlen=maxpadlen, padding='post')\n",
    "    \n",
    "    prediction = loaded_model_bert.predict(new_string) \n",
    "    \n",
    "    print(\"Toxicity levels for '{}':\".format(string))\n",
    "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "    print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "    print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxWkAax7JLF1",
    "outputId": "65369db1-bb72-4794-f0f9-2078c55fc9a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'go jump off a bridge jerk':\n",
      "Toxic:         89%\n",
      "Severe Toxic:  2%\n",
      "Obscene:       39%\n",
      "Threat:        0%\n",
      "Insult:        51%\n",
      "Identity Hate: 0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('go jump off a bridge jerk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLGZpQ1KIpFG",
    "outputId": "3644dc77-b74a-4e27-ac34-1e0f9c188ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'i will kill you':\n",
      "Toxic:         32%\n",
      "Severe Toxic:  1%\n",
      "Obscene:       3%\n",
      "Threat:        16%\n",
      "Insult:        2%\n",
      "Identity Hate: 0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('i will kill you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be1lw-liIpfe",
    "outputId": "a7b003b8-6868-4cc4-b152-6508524b88db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'have a nice day':\n",
      "Toxic:         1%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       0%\n",
      "Threat:        0%\n",
      "Insult:        0%\n",
      "Identity Hate: 0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('have a nice day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ-_6XzxIxxT",
    "outputId": "174cb383-9220-44d5-c659-4b5cf94c896d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'fuck ofF!!':\n",
      "Toxic:         99%\n",
      "Severe Toxic:  41%\n",
      "Obscene:       100%\n",
      "Threat:        0%\n",
      "Insult:        60%\n",
      "Identity Hate: 1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('fuck ofF!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s_5EQEFJD3G",
    "outputId": "b3a7a00a-a261-4c92-b6e5-37eeb9222a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'Hello, How are you?':\n",
      "Toxic:         1%\n",
      "Severe Toxic:  0%\n",
      "Obscene:       1%\n",
      "Threat:        0%\n",
      "Insult:        0%\n",
      "Identity Hate: 0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('Hello, How are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apOGOBU3uZxl",
    "outputId": "af67ae7a-affa-43d1-a066-12eb4519cd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'get the fuck away from me @sshole!!':\n",
      "Toxic:         99%\n",
      "Severe Toxic:  35%\n",
      "Obscene:       99%\n",
      "Threat:        1%\n",
      "Insult:        59%\n",
      "Identity Hate: 1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxicity_level('get the fuck away from me @sshole!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJ1LhR8Wg105"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7KKrp2Cg105"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wcGq1dF0iWju",
    "z8KPqb7_vC59",
    "D8hAcigh3Sh3",
    "NDWRHguXxOF_",
    "CeScalvZDEdD",
    "7wv1IdwZk_qG",
    "bXx4IOYn4_XB",
    "kJJJs5txVS8S",
    "7HkAldq1IJcr"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
